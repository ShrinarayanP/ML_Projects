{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1689267015306}],"collapsed_sections":["-Kee-DAl2viO"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Classification\n","##### **Contribution**    - Individual"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["**As We Know Health Insurance is an important financial decision which everyone must take. My Aim here is to observe which clients are willing to take Motor Insurance policy along with the heath Insurance policy as a part to increase my company's product sales.**"],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["**With the growing Market for Automobile, the demand of its insurance is also increasing and as my orgainsation has a product of motor insurance, its good opportunity to increase my sales.**\n","**Our first target customers are the one's who already are attached with us because of our other products.**\n","**Since I have the relational database and another product to sell, I need a tool which shortens my task to select the probable customer for motor Insurance.**"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import numpy as np # to use mathematical tools and arrays\n","import pandas as pd # to read and change data\n","import matplotlib.pyplot as plt # for visualization\n","import seaborn as sns # for visualization"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fe65F38rKcvC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","dataset = pd.read_csv(r'/content/drive/MyDrive/AIma Better/ML/Classification/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","dataset.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","dataset['Response'].value_counts()\n","# Its an imbalance dataset"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["print(f'Number of rows = {dataset.shape[0]}')\n","print(f'Number of columns = {dataset.shape[1]}')"],"metadata":{"id":"rQ7uLc--_Id9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","dataset.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","dataset.duplicated().sum()\n","# No duplicate values"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","dataset.isnull().sum()\n","# there is no null values"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","sns.heatmap(dataset.isnull())\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["1. my data has 12 columns and 381109 observations.\n","2. my data has no duplicates or any null values.\n","3. Its a mixed datset with categorical and numerical features.\n","4. Its a binary classification dataset with imbalanced output variables where 1 means customer is interested to take motor/vehicle insurance and 0 means not."],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","dataset.columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","dataset.describe(include = 'all')"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"code","source":["dataset.columns"],"metadata":{"id":"cx1fZnIBdqC3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**id** : Unique Identification of each customer\n","\n","**Gender** : Male/Female\n","\n","**Age** : Age of Customer\n","\n","**Driving_License** : If the customer has valid driving license or not\n","\n","**Region_Code** : The Region from which the customer belongs\n","\n","**Previously_Insured** : Whether the customer has any previous vehicle/motor Insurance\n","\n","**Vehicle_Age** : How old the vehicle is\n","\n","**Vehicle_Damage** : Past damage or present damage to vehicle\n","\n","**Annual_Premium** : Premium The customer need to pay for the policy\n","\n","**Policy_Sales_Channel** : Anonymous code for how the customer was reached eg. via agent or mail or call etc.\n","\n","**Vintage** : Customer association period with us.\n","\n","**Response** : whether the customer took Vehicle Insurance or not\n","\n","\n"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for i in dataset.columns:\n","  print(f'number of unique values in {i} = {dataset[i].nunique()}')"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","dataset.head() # to have a look at dataset in handy to analyse quicker"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Percent Male in total\n","dataset['Gender'].value_counts()['Male']*100/dataset['Gender'].value_counts().sum()"],"metadata":{"id":"9GUTyPPVbqsN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Percent Female in total\n","dataset['Gender'].value_counts()['Female']*100/dataset['Gender'].value_counts().sum()"],"metadata":{"id":"MzkPus78fRfh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ratio Male to Female\n","dataset['Gender'].value_counts()['Male']/dataset['Gender'].value_counts()['Female']"],"metadata":{"id":"v1OTT4TOfn-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.groupby('Gender')['Response'].value_counts()"],"metadata":{"id":"mSfvH0nmMNuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to see gender based conclusion, who conversts more often\n","df1 = pd.DataFrame(dataset.groupby('Gender')['Response'].value_counts()).rename(columns = {'Response':'Counts'}).reset_index()\n","df1"],"metadata":{"id":"itMCG0ukhDfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To check the Percent of time a male will accept the product\n","percent_male = int(df1[(df1['Gender'] == 'Male') & (df1['Response'] == 1)]['Counts'])/ \\\n"," (int(df1[(df1['Gender'] == 'Male') & (df1['Response'] == 1)]['Counts']) + \\\n","  int(df1[(df1['Gender'] == 'Male') & (df1['Response'] == 0)]['Counts'])) * 100\n","print(f'Percent of times males accepted vehicle insurance = {percent_male}')"],"metadata":{"id":"89dGS0VrMIdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To check the Percent of time a Female will accept the product\n","percent_female = int(df1[(df1['Gender'] == 'Female') & (df1['Response'] == 1)]['Counts'])/ \\\n"," (int(df1[(df1['Gender'] == 'Female') & (df1['Response'] == 1)]['Counts']) + \\\n","  int(df1[(df1['Gender'] == 'Female') & (df1['Response'] == 0)]['Counts'])) * 100\n","print(f'Percent of times females accepted vehicle insurance = {percent_female}')"],"metadata":{"id":"1sxiYyOrQd59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ratio of conversion Percent male to conversion percent female or ratio male to female who has converted.\n","percent_male/percent_female"],"metadata":{"id":"LlKsSgSzgiBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from above results we see that males are more prominent to accept the vehicle insurance"],"metadata":{"id":"N2qtktclRI4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = pd.DataFrame(dataset.groupby('Region_Code')['Response'].value_counts()).rename(columns = {'Response':'Counts'}).reset_index()\n","df2.head()"],"metadata":{"id":"dxWylKggFHim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df3 = dataset.groupby('Region_Code')['id'].count().sort_values(ascending = False).reset_index().rename(columns = {'id':'id_Count'})\n","df3['Cummulative_Percentage'] = df3['id_Count'].cumsum()*100/dataset.count()[0]\n","df3.head(10)"],"metadata":{"id":"Wjx_YTeRRehB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From Above it can be seen that most of my customer i.e. 28% belongs to the single region, with region code 28.\n","# This data shows me the region to focus on the most.\n","# This is also evident that my top 5 regions of total 53 regions has 50% of my customers. which also means my reach is not good in each region."],"metadata":{"id":"x5ZoOTtUU7wX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Since region Code 28 has most of my customers, lets study my response there\n","df2[df2['Region_Code'] == 28.0]"],"metadata":{"id":"5VsRw8f0FUxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lets study percent conversion in each region\n","l1 = []\n","for j in list(df3['Region_Code']):\n","  for i in list(df2['Region_Code']):\n","    if i == j:\n","      l1.append(int(df2[(df2['Region_Code'] == j) & (df2['Response'] == 1)]['Counts'])*100/int(df3[df3['Region_Code'] == j]['id_Count']))\n","      break\n","df3['Conversion_Percent'] = l1\n","df3.head()\n"],"metadata":{"id":"yPs5O1lobluA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lets see which has the highest Conversion Percent\n","df3.sort_values('Conversion_Percent', ascending = False).head()"],"metadata":{"id":"_gh6ws47yOgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From Above we find that although my reach is highest in region code 28 but my efficiency to Cross sell is highest in region code 38."],"metadata":{"id":"sTHLXS3gzxHy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the minmum and maximum age of the customers\n","min_age = dataset['Age'].min()\n","max_age = dataset['Age'].max()\n","print(f'Minimum Age in my dataset is : {min_age}')\n","print(f'Maximum Age in my dataset is : {max_age}')"],"metadata":{"id":"ahQkCwiPqjJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df4 = dataset[['Gender','Age','Response']]\n","df4['Age_Bins'] = pd.cut(dataset['Age'], [19,29,39,49,59,69,79,85], labels=['(19-29]','(29-39]','(39-49]','(49-59]','(59-69]','(69-79]','(79-85]'])"],"metadata":{"id":"sc4YTal6jNbO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to see which age band has highest number of conversions\n","df5 = df4[df4['Response'] == 1].groupby('Age_Bins')['Response'].count()\n","df5\n","# \"(\" = not included and \"]\" = included"],"metadata":{"id":"jTvvAsF4uhxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Since the age band 40 to 49 has highest count we will calculate the percent of total conversions\n","# the percent of conversions in the age band of 40 to 49 is ;\n","df5[2]*100/df5.sum()"],"metadata":{"id":"SJ-E7RNC7UuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.head()"],"metadata":{"id":"vcpulmye8ka0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df6 = pd.DataFrame(dataset.groupby('Vehicle_Damage')['Response'].value_counts()).rename(columns = {'Response':'Count'}).reset_index()\n","df6"],"metadata":{"id":"4KYHUp0cB-Lw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from above it is clear that if a customer has previous damage to vehicle, he or she will buy the motor insurance\n","# vd is abreviation for vehicle damage\n","vd_yes = df6.loc[3,'Count']/(df6.loc[2,'Count']+df6.loc[3,'Count'])*100\n","vd_no = df6.loc[1,'Count']/(df6.loc[0,'Count']+df6.loc[1,'Count'])*100\n","\n","print(f\"Percent conversion if previous damage is Yes : {vd_yes} %\")\n","print(f\"Percent conversion if previous damage is No : {vd_no} %\")"],"metadata":{"id":"GayKXRIB42jo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The above percent shows that in my total yes respondand for the motor insurance, is as follows\n","print(f'If yes responded for motor Insurance than there is previous damage to vehicle = {vd_yes/(vd_yes+vd_no)*100} %')"],"metadata":{"id":"NE-zABcM7eIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.set_option('display.max_rows', None)\n","a = dataset[dataset['Response'] == 1]['Policy_Sales_Channel'].value_counts()\n","b = dataset['Policy_Sales_Channel'].value_counts()\n","df7 = pd.merge(a,b,right_index = True, left_index = True).rename(columns = {'Policy_Sales_Channel_x':'Policy_Sales_Channel_R1','Policy_Sales_Channel_y':'Policy_Sales_Channel_All'})"],"metadata":{"id":"vEUDDqFR0f5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From Policy_Sales_Channel we see that code 26 is the one via which we have maximum yes response followed by code 124.\n","# Also from df7 we can see that channel code 160 is performing the worst as per sales because it is not efficient.\n","# We also Find that Channel Code 155 is performing the best as it has the highest efficiency.\n","# I have Found efficiency using the channel codes with response yes as numerator and channel code with all yes and no response as denominator.\n","df7['Channel_Efficiency'] = df7['Policy_Sales_Channel_R1'] * 100 / df7['Policy_Sales_Channel_All']\n","df7 = df7[df7['Policy_Sales_Channel_R1'] >= 25]\n","print(df7[df7['Channel_Efficiency'] == df7['Channel_Efficiency'].min()])\n","print(df7[df7['Channel_Efficiency'] == df7['Channel_Efficiency'].max()] )\n"],"metadata":{"id":"qSenYB71LKXU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["1. I grouped the data gender wise to see the valuations gender wise.\n","2. I grouped according to region code to get region wis valuations on customers count and my customers getting convrted.\n","3. I created a age band to see which age band is more into taking my Cross Sell product.\n","4. I also checked wheather previous damage to vehicles affects the probability of conversion.\n","5. I have also checked which channel code is most efficient way to get my sales incresed."],"metadata":{"id":"OdBd7cECbZVo"}},{"cell_type":"markdown","source":["\n","\n","1.   Males are 3% more willing to buy the motor insurance compared to females as per total Population count, the same is further verified when we take the ratio of whole population gender wise (Male to Female) is 1.177 and the same when considering the conversions is 1.33\n","2.   Most Number of clients are from Region Code 28 which consists of almost 28% of total.\n","3.   Though 28 region code has highest number of clients but my most conversions are from the region code 38 and region code 28 comes 2nd.\n","4.   I found with age band that the age group between 40 and 49 are 34.93% when we see my conversions.\n","5.   I found that if there is a previous damage to the vehicle than there is approximately 23% more chance that the customer will accept my product.\n","6.   I found that channel with code 160 is least efficient way of selling and Channel code 155 is the best way to sell my product.\n"],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","# visualizing gender wise response counts\n","ax = sns.barplot(x = df1['Gender'], y = df1['Counts'], hue = df1['Response'])\n","for i in ax.containers:\n","  ax.bar_label(i,)\n","plt.show()"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["**To visualise the gender wise distribution of response for the conversions.**"],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["***we can easily find that the male proportion who responded yes and overall is higher than the females population***"],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["**No negative but Males are a better target market compared to females.**"],"metadata":{"id":"9d72XMv86PDG"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# To visualize Gender Wise percent distribution in customers who responded yes\n","x1 = df1[df1['Response'] == 1]['Counts']\n","labels = df1['Gender'].value_counts().index\n","plt.figure(figsize = (7,5))\n","plt.pie(x1,labels = labels, autopct = '%1.1f%%',textprops={'fontsize': 8})\n","plt.show()"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["**Because pie chart is a best way to represent the percent in all.**"],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["**we can see that in the conversions, 61.1 % are male and 38.9 % are female.**"],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["**the above data helps me decide the target audience.**"],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["df6"],"metadata":{"id":"_O_6nsiB-ud8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chart - 3 visualization code\n","ax2 = sns.barplot(x = df6['Vehicle_Damage'], y = df6['Count'], hue = df6['Response'], palette='magma')\n","for i in ax2.containers:\n","  ax2.bar_label(i,)\n","plt.show()"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["**The above chart gives me a clear view of the affect if there is a previous damage to vehicles.**"],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["**I found that the previous damage has a huge effect on the customers to accept the motor insurance.**"],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["**Yes, The above information definitely helps me increase the sales by making us filter a more potential customer for the motor insurance cross selling product.**"],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","# visualizing region code wise population distribution.\n","plt.figure(figsize = (12,10))\n","ax1 = sns.barplot(y = sorted(dataset['Region_Code'].unique()), x = dataset.groupby('Region_Code')['id'].count().values, orient = 'h')\n","for i in ax1.containers:\n","  ax1.bar_label(i,)\n","plt.show()"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["**To see area wise distrbution of my client populations.**"],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["**We Can visualize that region code 28 is how big of my other areas in terms of customer base.**"],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["**Yes, the above information is very helpful to start from which area for cross selling of my product.**"],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["df5"],"metadata":{"id":"GaEWDx4PwOyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chart - 5 visualization code\n","labels1 = df5.index\n","plt.pie(df5.values, labels = labels1, autopct = '%1.2f%%', textprops={'fontsize': 7})\n","circle = plt.Circle((0,0),0.40,color = 'white')\n","p = plt.gcf()\n","p.gca().add_artist(circle)\n","plt.show()"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["**The Above chart is helping me to understand the age band within which clients have bought my product.**"],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["**We can see that most of my customers who responded yes are from age band of 29 to 49, which contribute to 58%+ of my total.**"],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["**The above graph makes it easy to know that age band 39 to 49 are more of my target base followed by 29 to 39 age band.**"],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","# To visualize if there is outliers in the Age feature.\n","sns.boxplot(dataset['Age'])\n","plt.yticks(np.arange(0,91,3))\n","plt.show()"],"metadata":{"id":"_XNv-1KWq4sh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UDcRx5pCTZEo"}},{"cell_type":"markdown","source":["**The above chart helps me find if there is any outliers in a feature.**"],"metadata":{"id":"8yiLDDeVTZEq"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"6OHMwvz3Twl3"}},{"cell_type":"markdown","source":["**We can see that there is no outlier in Age Column.**"],"metadata":{"id":"xLxrow-HTwmk"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"2LFXH83FUT6y"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","# To visualize The channel performance top 20 on response = 1.\n","df7.iloc[:,:-1].head(20).plot(kind = 'barh',stacked = True)\n","plt.show()"],"metadata":{"id":"sDiRND55UT7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"Qzt1TgAyUT7f"}},{"cell_type":"markdown","source":["**The above chart helps me see the proportion responded yes of all channel code wise.**"],"metadata":{"id":"aOxjKLTBUT7g"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"NeglANGcUT7g"}},{"cell_type":"markdown","source":["**We can see that channel code 160 has almost no response as yes while 26 is performing good.**"],"metadata":{"id":"u0bDVZlxUT7g"}},{"cell_type":"markdown","source":["#### Chart - 8 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# Correlation Heatmap visualization code\n","plt.figure(figsize=(8,6))\n","correlation = dataset.iloc[:,:].corr()\n","sns.heatmap(correlation, annot = True)\n","plt.show()"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["**The above chart shows me if there is any relation of any feature with the other.**"],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"-IaZMScRTm_G"}},{"cell_type":"markdown","source":["**There is no relation between any column with the other.**"],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 9 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","plt.figure(figsize=(6,4))\n","sns.pairplot(dataset)\n","plt.show()"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["**there are no missing values in my dataset**"],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["**No outliers.**"],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["dataset.head()"],"metadata":{"id":"WWAYWA8LJF9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"zeX52z-Bnwbw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['Vehicle_Damage'] = np.where(dataset['Vehicle_Damage'] == 'Yes', 1, 0)\n","dataset.drop('id', axis = 1, inplace = True)\n","dataset.drop('Driving_License', axis = 1, inplace = True)\n","dataset.head()"],"metadata":{"id":"wv41VhLStXfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode your categorical columns\n","X = dataset.iloc[:,:-1]\n","y = dataset.iloc[:,-1].values\n","\n","ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0,4])], remainder = 'passthrough')\n","X = ct.fit_transform(X)"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.tail()"],"metadata":{"id":"-7T5CLncj5ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.set_option('display.max_rows', 100)\n","print(pd.DataFrame(X))"],"metadata":{"id":"0zLDtKOHub1p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["**I have used OneHotEncoder for Vehicle_Age and Gender and Used conditions for yes and no, OneHotEncoders takes care if there is any new unique value in the feature and there may be a third gender and also we can have other values for Vehicle_Age.**"],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","**There is no Textual Analysis Needed in this dataset.**"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","\n","# I will not change my features as already we have seen the correlation matrix and we found that there is no correlation Between my features"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting\n","\n","# As there is no need for id and Driving_License in my feature as Driving_License does not help my sales,\n","# I Have deleted the column id, Driving_License from my dataset."],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["**I have just removed unwanted feature and rest I have kept as it is as there is no correlation and I do not find any feature is same or equivalent to the other.**"],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["**all my features are important for further study but id and Driving_License not being useful, i have removed them.**\n","**Rest all are somehow helping me to target the audience who will accept my product.**"],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data\n","from sklearn.preprocessing import StandardScaler\n","scale = StandardScaler()\n","X = scale.fit_transform(X)"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["**I Have used 80% data for training set and 20% for test because we need max number of data to overcome overfitting problem and enough numbers for testing, and this ratio is used widely for the above reason.**"],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["**Yes, my dataset is imbalance because of following.**"],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Why my dataset is imbalance.\n","# Because 12.256% of my data is yes and rest of this data is responded no(i.e. 97.744%).\n","dataset['Response'].value_counts().values[1]*100/(dataset['Response'].value_counts().values[0]+dataset['Response'].value_counts().values[1])"],"metadata":{"id":"eUcEtS8_kVXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)\n","# since my dataset is highly imbalance i have to handle this imbalance situation using oversampling technique.\n","import imblearn\n","from imblearn.over_sampling import SMOTE\n","sm = SMOTE(random_state = 1)\n","X_res, y_res = sm.fit_resample(X, y)"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we can see that my dataset is now balanced with equal number of yes and no.\n","no=0\n","yes = 0\n","for i in y_res:\n","    if i == 1:\n","        yes+=1\n","    else:\n","        no+=1\n","print('yes(1) = ', yes)\n","print('no(0) =', no)"],"metadata":{"id":"8hmzPPaQr4q9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["**I have used SMOTE method from imblearn library to oversample or create multiple number of new obsevations similar to the original data so that my resampled dataset will have equal number of responses as yes or no.**\n","\n","**I have used over sampling and not undersampling because decreasing the data can overfit my algorithm, while oversampling increases the data points to get better results.**"],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# Since i had to resample, lets split my new final dataset to study further.\n","X_res_train, X_res_test, y_res_train, y_res_test = train_test_split(X_res, y_res, test_size = 0.2, random_state = 1)"],"metadata":{"id":"dYcicWnztfyk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ML Model - 1 Implementation - Using Logistic Regression\n","from sklearn.linear_model import LogisticRegression\n","classifier1 = LogisticRegression(fit_intercept=True, random_state = 1)\n","# Fit the Algorithm\n","classifier1.fit(X_res_train, y_res_train)\n","# Predict on the model\n","y_pred = classifier1.predict(X_res_test)"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_train = classifier1.predict(X_res_train)\n","print('Test results')\n","print(f'f1_Score = {round(f1_score(y_pred, y_res_test)*100,2)}%')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred, y_res_test)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred, y_res_test)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred, y_res_test)*100,2)}%')\n","print()\n","print('Training results')\n","print(f'f1_Score = {round(f1_score(y_pred_train, y_res_train)*100,2)} %')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred_train, y_res_train)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred_train, y_res_train)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred_train, y_res_train)*100,2)}%')"],"metadata":{"id":"MupO6BbrN7xI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From Above we can see that my accuracy Score is 78.32 % only\n","# and My precision Score is 97.35%\n","X_res_train2,X_res_test2, y_res_train2, y_res_test2 = train_test_split(X_res, y_res, test_size = 0.2, random_state = 1)"],"metadata":{"id":"znT80gNXJ8lX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","from sklearn.model_selection import GridSearchCV\n","param_grid = [\n","    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n","    'C' : [0.001,0.01,0.1,1,10],\n","     'fit_intercept' : [True, False],\n","     'solver' : ['liblinear'],\n","     'max_iter' : [1000]\n","    }\n","]\n","clf1 = LogisticRegression(random_state = 1)\n","classifier2 = GridSearchCV(clf1, param_grid = param_grid, scoring= 'accuracy', cv = 5, verbose = True, n_jobs = -1)\n","# Fit the Algorithm\n","classifier2.fit(X_res_train2,y_res_train2)"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier2.best_estimator_"],"metadata":{"id":"58qUKOL1Ct5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier2.best_params_"],"metadata":{"id":"LPlPUglxDMwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier2.best_score_"],"metadata":{"id":"BMc1LgSwD2kT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the evaluation matrix for model 2\n","y_pred2 = classifier2.predict(X_res_test)\n","y_pred2_train = classifier2.predict(X_res_train)\n","print('Test results')\n","print(f'f1_Score = {round(f1_score(y_pred2, y_res_test2)*100,2)}%')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred2, y_res_test2)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred2, y_res_test2)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred2, y_res_test2)*100,2)}%')\n","print()\n","print('Training results')\n","print(f'f1_Score = {round(f1_score(y_pred2_train, y_res_train2)*100,2)} %')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred2_train, y_res_train2)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred2_train, y_res_train2)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred2_train, y_res_train2)*100,2)}%')"],"metadata":{"id":"1eEI6VVRQxze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["**I have used GridSearchCV, because it runs all possible cases.**"],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["**There is very minute difference.**"],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"code","source":["# ML Model - 2 : Using ensemble method RandomForestClassifier\n","from sklearn.ensemble import RandomForestClassifier  # importing the needed library\n","classifier3 = RandomForestClassifier()\n","classifier3.fit(X_res_train, y_res_train)"],"metadata":{"id":"pIQ3RTr5Q7l8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred3 = classifier3.predict(X_res_test)\n","y_pred3_train = classifier3.predict(X_res_train)"],"metadata":{"id":"IxpQwpl1WQYz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","print('Test results')\n","print(f'f1_Score = {round(f1_score(y_pred3, y_res_test)*100,2)}%')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred3, y_res_test)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred3, y_res_test)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred3, y_res_test)*100,2)}%')\n","print()\n","print('Training results')\n","print(f'f1_Score = {round(f1_score(y_pred3_train, y_res_train)*100,2)} %')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred3_train, y_res_train)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred3_train, y_res_train)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred3_train, y_res_train)*100,2)}%')"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We see from above that my model is overfit since my training set accuracy is far higher than the test set accuracy"],"metadata":{"id":"h-Bccsexa4UD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","from sklearn.model_selection import RandomizedSearchCV\n","param_grid2 = {'max_depth' : [3,4,5],\n","               'n_estimators':[50,100,200],\n","               'min_samples_leaf' : [11,15,19],\n","               'criterion' : ['gini','entropy']\n","\n","}\n","classifier4 = RandomizedSearchCV(classifier3,param_distributions = param_grid2, scoring = 'accuracy', n_jobs = -1, cv = 5, verbose = True)\n","# Fit the Algorithm\n","classifier4.fit(X_res_train, y_res_train)\n","# Predict on the model\n","y_pred4 = classifier4.predict(X_res_test)\n","y_pred4_train = classifier4.predict(X_res_train)"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier4.best_params_"],"metadata":{"id":"RdcaIvFr6jFr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Test results')\n","print(f'f1_Score = {round(f1_score(y_pred4, y_res_test)*100,2)}%')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred4, y_res_test)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred4, y_res_test)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred4, y_res_test)*100,2)}%')\n","print()\n","print('Training results')\n","print(f'f1_Score = {round(f1_score(y_pred4_train, y_res_train)*100,2)} %')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred4_train, y_res_train)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred4_train, y_res_train)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred4_train, y_res_train)*100,2)}%')"],"metadata":{"id":"A0Inb_sr6JKJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["**I have used RandomizedSearchCV Because it will take lesser time to get the HyperParameters tuned compared to GridSearchCV. **"],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["**Yes, I saw 2% improvement in my accuracy Score also I got a perfectly fitted score as compared to before where My model was overfit.**"],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","from sklearn.ensemble import GradientBoostingClassifier\n","classifier5 = GradientBoostingClassifier()\n","\n","# Fit the Algorithm\n","classifier5.fit(X_res_train, y_res_train)\n","# Predict on the model\n","y_pred5 = classifier5.predict(X_res_test)\n","y_pred5_train = classifier5.predict(X_res_train)"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","print('Test results')\n","print(f'f1_Score = {round(f1_score(y_pred5, y_res_test)*100,2)}%')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred5, y_res_test)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred5, y_res_test)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred5, y_res_test)*100,2)}%')\n","print()\n","print('Training results')\n","print(f'f1_Score = {round(f1_score(y_pred5_train, y_res_train)*100,2)} %')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred5_train, y_res_train)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred5_train, y_res_train)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred5_train, y_res_train)*100,2)}%')"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","param_grid3= {'loss':['log_loss','exponential'],\n","              'learning_rate':[0.001,0.01,0.1],\n","              'n_estimators' : [25,50],\n","              'min_samples_leaf' : [5,7,9]\n","              }\n","classifier6 = RandomizedSearchCV(estimator = classifier5, param_distributions = param_grid3, cv= 5, n_jobs = -1, verbose = True, scoring = 'accuracy')\n","# Fit the Algorithm\n","classifier6.fit(X_res_train, y_res_train)\n","# Predict on the model\n","y_predict6 = classifier6.predict(X_res_test)\n","y_pred6_train = classifier6.predict(X_res_train)"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier6.best_params_"],"metadata":{"id":"Ru4L34oUVm0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","print('Test results')\n","print(f'f1_Score = {round(f1_score(y_predict6, y_res_test)*100,2)}%')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_predict6, y_res_test)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_predict6, y_res_test)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_predict6, y_res_test)*100,2)}%')\n","print()\n","print('Training results')\n","print(f'f1_Score = {round(f1_score(y_pred6_train, y_res_train)*100,2)} %')\n","print(f'Confusion Matrix ; \\n {confusion_matrix(y_pred6_train, y_res_train)}')\n","print(f'Accuracy_Score = {round(accuracy_score(y_pred6_train, y_res_train)*100,2)}%')\n","print(f'Precision_Score = {round(precision_score(y_pred6_train, y_res_train)*100,2)}%')"],"metadata":{"id":"HH6S3cWNVGVm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["**I have used RandomizedSearchCV because it gives faster tuned results.**"],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["**no improvement after hypertuning, default one was better.**"],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["**I have used accuracy and precision score along with f1_score as evaluation metric because it is a classification problem**"],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["**I will use default GradientBoostingClassifier as my model for the given data because it gives me the best evaluation metrics results**"],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["**I have used Default GradientBoostingClassifier as my model for the given dataset, This algorithm works on reducing the error on each iteration, It first fits the X(features) with y(target) then it will try to check the errors in my result and will reduce those further.**"],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"code","source":["classifier5.feature_importances_"],"metadata":{"id":"PuP_WRBLmQxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualising The selected model feature importance\n","ll1 = ['Female','Male','V_Age_1-2 Year','V_Age_<1 Year', 'V_Age_>2 Years','Age','Region','Previously_insured','V_Damage','Premium','Policy_Channel','Vintage']\n","plt.barh(ll1, classifier5.feature_importances_)\n","plt.xlabel('Feature Importance')\n","plt.ylabel('Feature')\n","plt.title('Feature Importance Based on Deafault GradientBoostingClassifier'.title())\n","plt.show()"],"metadata":{"id":"FI43-cj0goLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From Above we can see that Vehicle Damage Has the Hoghest Impact on my dataset followed by Previously_insured and Members age."],"metadata":{"id":"UiroziuSm0Yy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["**From EDA Studies**\n","1. We Found that males buy more no. of Motor Insurances.\n","2. My most percent conversions are from region 38.\n","3. Age Band of 40-49 are most buyers.\n","4. If there is previous damage to vehicle then we see that the motor insurance buying increses sharply.\n","5. Policy Sales Channel also had impact on increasing the sales.\n","\n","**From ML Algorithms Studies**\n","1. We saw that simple RandomForestClassifier is giving a overfit results.\n","2. Default GradientBoostingClassifier is giving me the best results while the HyperParameterTuned GradientBoostingClassifier is not giving me any better results.\n","3. After studying the feature importance we also found that Previous Damage is affecting my model the most which was also shown in my **EDA Study**."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}